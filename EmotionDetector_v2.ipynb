{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "EmotionDetector_v2.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIbvZr97v0fX",
        "colab_type": "text"
      },
      "source": [
        "# Using LittleVGG for Emotion Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3FiuAWzwz2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQUClXPew4wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lPcyHkhv0fZ",
        "colab_type": "text"
      },
      "source": [
        "### Training Emotion Detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJFsmn7Sv0fb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdcf9366-75d9-4d10-ce3e-30b29c54d14d"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import regularizers\n",
        "from keras.regularizers import l1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoxr7DTAv0fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## initializing "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8mxxKEyx7dL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "55fd3326-0726-4ffa-d951-9d6456bf97e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "#%cd /content/drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9OnckGiv0fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 7\n",
        "img_rows, img_cols = 48, 48\n",
        "batch_size = 512\n",
        "\n",
        "train_data_dir = '/gdrive/My Drive/Colab Notebooks/fer2013/train'\n",
        "validation_data_dir = '/gdrive/My Drive/Colab Notebooks/fer2013/validation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNei7KsAv0fy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5e92b6de-66f2-4ae2-ea8f-0e1502676aaa"
      },
      "source": [
        "# Let's use some data augmentaiton \n",
        "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "      rotation_range=30,\n",
        "      shear_range=0.3,\n",
        "      zoom_range=0.3,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 24256 images belonging to 5 classes.\n",
            "Found 3006 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASjKLaAXv0gA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4ee0e35-7647-4565-ec75-793bd7badc81"
      },
      "source": [
        "print(validation_generator.class_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Angry': 0, 'Happy': 1, 'Neutral': 2, 'Sad': 3, 'Surprise': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cqBdpc2v0gI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "3a8303a1-5b38-4f73-b2f7-21fcac7f0d3f"
      },
      "source": [
        "# Create the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',kernel_regularizer=regularizers.l2(0.0001),input_shape=(48,48,1)))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(5, kernel_size=(1, 1), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "# # model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(5, kernel_size=(4, 4), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.summary()\n",
        "# model.add(Dense(1024, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(7, activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 46, 46, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 44, 44, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 20, 20, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 5)           645       \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 1, 1, 5)           405       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 241,306\n",
            "Trainable params: 241,306\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsoHStlHv0gO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzJevTeHv0gY",
        "colab_type": "text"
      },
      "source": [
        "## Training our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR1zvZxtzHBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0709dc0a-ffea-4531-c523-4a521b504410"
      },
      "source": [
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: torch-0.4.1-{platform}-linux_x86_64.whl is not a valid wheel filename.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSB_5IoQv0gb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "fa74fbb6-b094-466d-edfa-c8d489340e25"
      },
      "source": [
        "# If you want to train the same model or try other models, go for this\n",
        "import torch\n",
        "model_save_name = 'model_v6_{epoch}.hdf5'\n",
        "filepath = F\"/gdrive/My Drive/Colab Notebooks/{model_save_name}\"\n",
        "\n",
        "#filepath = os.path.join(\"./emotion_detector_models/model_v6_{epoch}.hdf5\")\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                             monitor='val_acc',\n",
        "                                             verbose=1,\n",
        "                                             save_best_only=True,\n",
        "                                             mode='max')\n",
        "callbacks = [checkpoint]\n",
        "# if mode == \"train\":\n",
        "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "nb_train_samples = 28709\n",
        "nb_validation_samples = 3589\n",
        "epochs = 150\n",
        "model_info = model.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch=nb_train_samples // batch_size,\n",
        "            epochs=epochs,\n",
        "            callbacks = callbacks,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=nb_validation_samples // batch_size)\n",
        "\n",
        "# plot_model_history(model_info)\n",
        "# model.save_weights('model.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 22 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 1/56 [..............................] - ETA: 4:48:04 - loss: 1.6363 - accuracy: 0.2363"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 43 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2/56 [>.............................] - ETA: 4:30:21 - loss: 1.6361 - accuracy: 0.2285"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 27 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3/56 [>.............................] - ETA: 4:30:09 - loss: 1.6356 - accuracy: 0.2266"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 2 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 5/56 [=>............................] - ETA: 3:27:23 - loss: 1.6358 - accuracy: 0.2148"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 11 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6/56 [==>...........................] - ETA: 3:30:20 - loss: 1.6356 - accuracy: 0.2100"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 46 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7/56 [==>...........................] - ETA: 3:33:07 - loss: 1.6354 - accuracy: 0.2065"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 34 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8/56 [===>..........................] - ETA: 3:31:45 - loss: 1.6353 - accuracy: 0.2063"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 33 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz-j0eiu1qQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI6HWKD5v0gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model_info.history.keys())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(model_info.history['loss'])\n",
        "plt.plot(model_info.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCe69Pj_v0gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(os.path.join(\"./emotion_detector_models/model_v6_146.hdf5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaPhZnLVv0gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_info = model.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch=nb_train_samples // batch_size,\n",
        "            epochs=epochs,\n",
        "            callbacks = callbacks,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=nb_validation_samples // batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL__UiOYv0g7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(model_info.history['loss'])\n",
        "plt.plot(model_info.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nVNScGVv0hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(os.path.join(\"./emotion_detector_models/model_29.hdf5\"))\n",
        "# history = model.fit_generator(\n",
        "#     train_generator,\n",
        "#     steps_per_epoch = nb_train_samples // batch_size,\n",
        "#     epochs = epochs,\n",
        "#     callbacks = callbacks,\n",
        "#     validation_data = validation_generator,\n",
        "#     validation_steps = nb_validation_samples // batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8rBbixcv0hI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWcrlSPRv0hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "# Found 28709 images belonging to 7 classes.\n",
        "# Found 3589 images belonging to 7 classes.\n",
        "\n",
        "\n",
        "# nb_train_samples = 28273\n",
        "# nb_validation_samples = 3534\n",
        "nb_train_samples = 28709\n",
        "nb_validation_samples = 3589\n",
        "\n",
        "# We need to recreate our validation generator with shuffle = false\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "class_labels = validation_generator.class_indices\n",
        "class_labels = {v: k for k, v in class_labels.items()}\n",
        "classes = list(class_labels.values())\n",
        "\n",
        "#Confution Matrix and Classification Report\n",
        "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(validation_generator.classes, y_pred))\n",
        "print('Classification Report')\n",
        "target_names = list(class_labels.values())\n",
        "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
        "\n",
        "plt.imshow(cnf_matrix, interpolation='nearest')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
        "_ = plt.yticks(tick_marks, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvqgvTKMv0hU",
        "colab_type": "text"
      },
      "source": [
        "### Loading our saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPQELycQv0hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "classifier = load_model('./emotion_detector_models/model_v3_71.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS8Wzr7Xv0hd",
        "colab_type": "text"
      },
      "source": [
        "### Get our class labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuiLS1Kqv0he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "class_labels = validation_generator.class_indices\n",
        "class_labels = {v: k for k, v in class_labels.items()}\n",
        "classes = list(class_labels.values())\n",
        "print(class_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaUmd3D8v0hl",
        "colab_type": "text"
      },
      "source": [
        "### Let's test on some of validation images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ani7BN2bv0hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "\n",
        "def draw_test(name, pred, im, true_label):\n",
        "    BLACK = [0,0,0]\n",
        "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
        "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
        "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
        "    cv2.imshow(name, expanded_image)\n",
        "\n",
        "\n",
        "def getRandomImage(path, img_width, img_height):\n",
        "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
        "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
        "    random_directory = np.random.randint(0,len(folders))\n",
        "    path_class = folders[random_directory]\n",
        "    file_path = path + path_class\n",
        "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
        "    random_file_index = np.random.randint(0,len(file_names))\n",
        "    image_name = file_names[random_file_index]\n",
        "    final_path = file_path + \"/\" + image_name\n",
        "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
        "\n",
        "# dimensions of our images\n",
        "img_width, img_height = 48, 48\n",
        "\n",
        "# We use a very small learning rate \n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(lr = 0.001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "files = []\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "# predicting images\n",
        "for i in range(0, 10):\n",
        "    path = './fer2013/validation/' \n",
        "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
        "    files.append(final_path)\n",
        "    true_labels.append(true_label)\n",
        "    x = image.img_to_array(img)\n",
        "    x = x * 1./255\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    images = np.vstack([x])\n",
        "    classes = model.predict_classes(images, batch_size = 10)\n",
        "    predictions.append(classes)\n",
        "    \n",
        "for i in range(0, len(files)):\n",
        "    image = cv2.imread((files[i]))\n",
        "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
        "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qNSYFkYv0hq",
        "colab_type": "text"
      },
      "source": [
        "### Test on a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng0zk4iCv0hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "def face_detector(img):\n",
        "    # Convert image to grayscale\n",
        "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
        "    if faces is ():\n",
        "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
        "    \n",
        "    allfaces = []   \n",
        "    rects = []\n",
        "    for (x,y,w,h) in faces:\n",
        "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
        "        allfaces.append(roi_gray)\n",
        "        rects.append((x,w,y,h))\n",
        "    return rects, allfaces, img\n",
        "\n",
        "img = cv2.imread(\"rajeev.jpg\")\n",
        "rects, faces, image = face_detector(img)\n",
        "\n",
        "i = 0\n",
        "for face in faces:\n",
        "    roi = face.astype(\"float\") / 255.0\n",
        "    roi = img_to_array(roi)\n",
        "    roi = np.expand_dims(roi, axis=0)\n",
        "\n",
        "    # make a prediction on the ROI, then lookup the class\n",
        "    preds = classifier.predict(roi)[0]\n",
        "    label = class_labels[preds.argmax()]   \n",
        "\n",
        "    #Overlay our detected emotion on our pic\n",
        "    label_position = (rects[i][0] + int((rects[i][1]/2)), abs(rects[i][2] - 10))\n",
        "    i =+ 1\n",
        "    cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
        "    \n",
        "cv2.imshow(\"Emotion Detector\", image)\n",
        "cv2.waitKey(0)\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYqfWRMxv0hu",
        "colab_type": "text"
      },
      "source": [
        "### Let's try this on our webcam\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK9MOIgHv0hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "def face_detector(img):\n",
        "    # Convert image to grayscale\n",
        "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
        "    if faces is ():\n",
        "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
        "    \n",
        "    for (x,y,w,h) in faces:\n",
        "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "\n",
        "    try:\n",
        "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
        "    except:\n",
        "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
        "    return (x,w,y,h), roi_gray, img\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "    rect, face, image = face_detector(frame)\n",
        "    if np.sum([face]) != 0.0:\n",
        "        roi = face.astype(\"float\") / 255.0\n",
        "        roi = img_to_array(roi)\n",
        "        roi = np.expand_dims(roi, axis=0)\n",
        "\n",
        "        # make a prediction on the ROI, then lookup the class\n",
        "        preds = classifier.predict(roi)[0]\n",
        "        label = class_labels[preds.argmax()]  \n",
        "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
        "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
        "    else:\n",
        "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
        "        \n",
        "    cv2.imshow('All', image)\n",
        "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
        "        break\n",
        "        \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zud2kyLYv0h1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap.release()\n",
        "cv2.destroyAllWindows()      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc5lvALcv0h4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}